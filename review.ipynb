{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "# Обзор научных работ по StyleGAN\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом обзоре я рассмотрю две смежные работы, предлагающих методы обучения нейронной сети  \n",
    "StyleGAN, представленной в [Analyzing and Improving the Image Quality of StyleGAN](https://arxiv.org/abs/1912.04958), для генерации и  \n",
    "редактирования изображений с помощью текста. Они опираются на использование модели  \n",
    "CLIP ([Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020)) от OpenAI, позволяющей  \n",
    "отображать изображения и текстовые описания в одно семантически связное линейное пространство."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StyleCLIP\n",
    "Ссылка на статью: https://arxiv.org/abs/2103.17249\n",
    "\n",
    "Авторы работы предлагают три метода редактирования изображений:\n",
    "1. **Latent Optimization**:  \n",
    "    - Оптимизация в пространстве $\\mathcal{W}^+$ для минимизации составного CLIP-loss  \n",
    "    - Тензор $\\mathcal{W}^+$ состоит из 18 копий базового вектора $\\mathcal{W}$ размерности 512\n",
    "    - Метод не требует предварительного обучения, но вывод модели (этап оптимизации латента) долгий –  \n",
    "      в работе указано 98с для NVIDIA GTX 1080Ti на 200-300 итерациях\n",
    "    - На практике ожидается подбор коэффициентов функции потерь вручную в каждом отдельном случае,  \n",
    "      т.к. их оптимальный выбор зависит от целевого изображения и характера изменений\n",
    "\n",
    "2. **Latent Mapper**:  \n",
    "    - Обучение маппера $M: \\mathcal{W}^+ \\rightarrow \\mathcal{W}^+$\n",
    "    - Маппер состоит из трёх полносвязных сетей, каждая из которых преобразуют часть набора  \n",
    "      входных латентов, отвечающую в генераторе за свой уровень деталей изображения\n",
    "    - В зависимости от задачи и промпта, может быть достаточно обучить только часть маппера\n",
    "    - Применяется для конкретного типа манипуляции\n",
    "    - Более быстрый на этапе вывода подход\n",
    "\n",
    "3. **Global Directions**:  \n",
    "    - Поиск глобальных направлений в $\\mathcal{W}^+$ и в итоге – в $\\mathcal{S}$\n",
    "    - Линейная интерполяция вдоль этих направлений\n",
    "    - Адаптированная версия этого подхода применяется в StyleGAN-NADA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Векторы и их значение:\n",
    "- $t$ (text) - эмбеддинг целевого промпта в текстовом подпространстве CLIP $\\mathcal{T}$\n",
    "- $\\Delta t$ - разница между эмбеддингами целевого и исходного описаний\n",
    "- $i$ (image) - эмбеддинг исходного изображения в подпространстве CLIP для изображений $\\mathcal{I}$\n",
    "- $\\Delta i$ - разница между эмбеддингами целевого и исходного изображений в $\\mathcal{I}$\n",
    "- $s$ - \"style code\", набор параметров слоёв генератора для исходного изображения в стилевом  \n",
    "    пространстве StyleGAN – $\\mathcal{S}$\n",
    "- $\\Delta s$ - искомое изменение этого объекта для изображения в процессе оптимизации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Технически основной задачей метода **Global Directions** является отображение вектора $\\Delta t$ в $\\Delta s$ для  \n",
    "создания целевого изображения $G(s + \\alpha\\Delta s)$, где $\\alpha$ – коэффициент скорости движения в нужную  сторону  \n",
    "в стилевом пространстве $\\mathcal{S}$.\n",
    "\n",
    "$\\Delta t$ формируется следующим образом:\n",
    "1. С помощью **ImageNet prompt bank** формируется определённое кол-во пропмтов описания исходного  \n",
    "    изображения, затем их CLIP-эмбеддинги усредняются для более стабильного результата\n",
    "2. То же применяется к целевому промпту\n",
    "3. Нормализованная разница между получившимися векторами и есть $\\Delta t$\n",
    "\n",
    "Далее вычисляется, как поэлементое изменение вектора $s$ влияет на изменение эмбеддинга  \n",
    "изображения $\\Delta i$ (коллинеарного $\\Delta t)$, в итоге формируется искомый $\\Delta s$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "## StyleGAN-NADA\n",
    "Ссылка на статью: https://arxiv.org/abs/2108.00946\n",
    "\n",
    "Цель у StyleGAN-NADA та же, что у StyleCLIP – реализовать метод редактирования изображений исключительно  \n",
    "с помощью текста, используя предобученные модели StyleGAN и CLIP. Основная идея – в дообучении генератора  \n",
    "StyleGAN, ориентируясь на вывод CLIP в ходе оптимизации подмножества параметров модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выбор функционала потерь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В начале обсуждения своего метода авторы размышляют о наивной функции потерь, использующей косинусное  \n",
    "расстояние между эмбеддингами изображения и текста в пространстве CLIP:\n",
    "$$\n",
    "\\mathcal{L}_{global} = D_{CLIP}\\big[G(w), t_{target}\\big]\n",
    "$$\n",
    "\n",
    "Они приходят в выводу, что такой лосс в чистом виде мало применим в качестве основного в силу склонности  \n",
    "получаемой модели к артефактам и однородныму выводу (т.н. mode collapse). Однако $D_{CLIP}$ всё же используется  \n",
    "в работе далее – для выбора обучаемых слоёв генератора."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее авторы черпают вдохновение в третьем методе StyleCLIP, который заключается в поиске направления от  \n",
    "источника к цели в пространстве CLIP. Теперь цель – обучение генератора таким образом, чтобы порождённые  \n",
    "им изображения отличались от источника исключительно в этом направлении. Авторы предлагают функционал  \n",
    "потерь $\\mathcal{L}_{direction}$, опирающийся на разницу $∆T$ эмбеддингов CLIP целевого и исходного текстов и разницу $∆I$ для  \n",
    "изображений, порождаемых обучаемой версией генератора и исходной, которая не подвергается изменениям:\n",
    "$$\\\\[2em]\n",
    "∆T = E_{T} (t_{target} ) − E_{T}(t_{source} )\n",
    "$$\n",
    "$$\\\\[2em]\n",
    "∆I = E_{I} (G_{train} (w)) − E_{I} (G_{frozen} (w))\n",
    "$$\n",
    "$$\\\\[2em]\n",
    "\\mathcal{L}_{direction} = 1 − \\frac{∆I · ∆T}{|∆I| |∆T|}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Заморозка слоёв генератора\n",
    "\n",
    "В работе авторы столкнулись с проблемами переобучения и схлопывания мод распределения (mode collapse)  \n",
    "при достаточно долгой оптимизации, которая, однако, необходима при существенном изменении домена изображений  \n",
    "(напр. из фото в скетч). Для борьбы с этим авторы опираются на ранние работы коллег и предлагают использовать  \n",
    "ограничение обучаемых параметров, а именно – целых свёрточных слоёв генератора, оставляя лишь те, что наиболее  \n",
    "сильно влияют на перевод изображения в целевой домен. Такой подход снижает сложность модели и уменьшает  \n",
    "риск переобучения, к тому же ограничивая кол-во вычислений.\n",
    "\n",
    "Для выбора наиболее значимых слоёв авторы предлагают использовать метод **Latent Optimization** из StyleCLIP,  \n",
    "однако целью в этом случае является не сама оптимизация латента $\\mathcal{W}^+$ – а наблюдение, какие его части (векторы) более  \n",
    "всего изменились. Слои генератора, которые им соответствуют, далее обучаются, остальные \"замораживаются\".  \n",
    "Также не подвергаются оптимизации **Mapping network** – полносвязная сеть в начале генератора, его части, отвечающие  \n",
    "за аффинные преобразования, и слои **toRGB**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение маппера\n",
    "\n",
    "В случае сильных изменений домена авторы замечают, что обучившийся генератор иногда порождает изображения из  \n",
    "обоих – нового и старого доменов. Чтобы этого избежать, они используют метод **Latent Mapper** из StyleCLIP,  \n",
    "однако замечают в дополнении, что это может приводить к появлению характерных артефактов. Авторы видят их  \n",
    "корреляцию с нормой векторов в пространстве CLIP. Для борьбы с ними предлагается использовать следующую  \n",
    "регуляризацию на ограничение этой нормы:\n",
    "$$\\\\[2em]\n",
    "L_{norm} = |E_I(G(w)) - E_I(G(M(w)))|^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***\n",
    "Это основные идеи статей. Далее их авторы приводят графики и примеры изображений, созданных с помощью  \n",
    "обученных их способами моделей – вполне убедительные – и сравнивают результаты с другими методами,  \n",
    "демонстрируя практическую ценность своих работ."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
