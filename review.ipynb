{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "# Обзор научных работ по StyleGAN\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Методы StyleCLIP\n",
    "1. **Latent Optimization**:  \n",
    "    - Оптимизация в пространстве $\\mathcal{W}^+$ для минимизации составного CLIP-loss  \n",
    "    - $\\mathcal{W}^+$ состоит из 18 копий базового вектора $\\mathcal{W}$ размерности 512\n",
    "    - Не требует предварительного обучения, но вывод модели (этап оптимизации латента) долгий –  \n",
    "      в работе указано 98с для NVIDIA GTX 1080Ti на 200-300 итерациях\n",
    "    - На практике ожидается подбор коэффициентов функции потерь вручную в каждом отдельном случае,  \n",
    "      т.к. их оптимальный выбор зависит от целевого изображения и характера изменений\n",
    "\n",
    "2. **Latent Mapper**:  \n",
    "    - Обучение маппера $M: \\mathcal{W}^+ \\rightarrow \\mathcal{W}^+$\n",
    "    - Маппер состоит из трёх полносвязных сетей, каждая из которых преобразуют часть набора  \n",
    "      входных латентов, отвечающую в генераторе за свой уровень деталей изображения\n",
    "    - В зависимости от задачи и промпта, может быть достаточно обучить только часть маппера\n",
    "    - Применяется для конкретного типа манипуляции\n",
    "    - Более быстрый на этапе вывода подход\n",
    "\n",
    "3. **Global Directions**:  \n",
    "    - Поиск глобальных направлений в $\\mathcal{W}^+$ и в итоге – в $\\mathcal{S}$\n",
    "    - Линейная интерполяция вдоль этих направлений\n",
    "    - Адаптированная версия этого подхода применяется в StyleGAN-NADA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Векторы и их значение:\n",
    "- $t$ (text) - эмбеддинг целевого промпта в текстовом подпространстве CLIP $\\mathcal{T}$\n",
    "- $\\Delta t$ - разница между эмбеддингами целевого и исходного описаний\n",
    "- $i$ (image) - эмбеддинг исходного изображения в подпространстве CLIP для изображений $\\mathcal{I}$\n",
    "- $\\Delta i$ - разница между эмбеддингами целевого и исходного изображений в $\\mathcal{I}$\n",
    "- $s$ - \"style code\", набор параметров слоёв генератора для исходного изображения в стилевом  \n",
    "    пространстве StyleGAN – $\\mathcal{S}$\n",
    "- $\\Delta s$ - искомое изменение этого объекта для изображения в процессе оптимизации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задачей метода **Global Directions** является отображение вектора $\\Delta t$ в $\\Delta s$ для создания целевого  \n",
    "изображения $G(s + \\alpha\\Delta s)$, где $\\alpha$ – коэффициент скорости движения в нужную сторону в стилевом пространстве $\\mathcal{S}$.\n",
    "\n",
    "$\\Delta t$ формируется следующим образом:\n",
    "1. С помощью ImageNet prompt bank формируется определённое кол-во эмбеддингов описания исходного изображения,  \n",
    "    затем они усредняются для более стабильного результата\n",
    "2. То же применяется к целевому промпту\n",
    "3. Нормализованная разница между этими векторами и есть $\\Delta t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "## Метод StyleGAN-NADA\n",
    "\n",
    "Цель у StyleGAN-NADA та же, что у StyleCLIP – реализовать метод редактирования изображений исключительно  \n",
    "с помощью текста, используя предобученные модели StyleGAN и CLIP. Основная идея – в дообучении генератора  \n",
    "StyleGAN, ориентируясь на вывод CLIP в ходе оптимизации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В начале обсуждения своего метода авторы размышляют о наивной функции потерь, использующей косинусное  \n",
    "расстояние между эмбеддингами изображения и текста в пространстве CLIP:\n",
    "$$\n",
    "\\mathcal{L}_{global} = D_{CLIP}\\big[G(w), t_{target}\\big]\n",
    "$$\n",
    "Они приходят в выводу, что такой лосс в чистом виде мало применим в качестве основного в силу склонности  \n",
    "получаемой модели к артефактам и однородныму выводу (т.н. mode collapse). Однако $D_{CLIP}$ всё же используется  \n",
    "в работе далее – для выбора обучаемых слоёв генератора."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее авторы черпают вдохновение в третьем методе StyleCLIP, который заключается в поиске направления от  \n",
    "источника к цели в пространстве CLIP. Теперь цель – обучение генератора таким образом, чтобы порождённые  \n",
    "им изображения отличались от источника исключительно в этом направлении."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
