{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализация StyleGAN-NADA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Клонируем эталонную реализацию StyleGAN2 для PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/NVlabs/stylegan2-ada-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import clip\n",
    "import dnnlib\n",
    "import pretrained_networks\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from training.networks import Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Загрузка моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=\"cuda\")\n",
    "stylegan_network = pretrained_networks.load_network(\"path_to_stylegan_pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Допустим, у вас клонирован репозиторий StyleGAN2-ADA (https://github.com/NVlabs/stylegan2-ada-pytorch)\n",
    "# В нем есть модули, отвечающие за тренинг, загрузку сетей, расширения данных и т. д.\n",
    "# Ниже - условные внутрипроектные импорты (примерные названия, в реальном коде могут отличаться):\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Основные идеи:\n",
    "# 1. У нас есть два текстовых описания: исходный домен (src_text) и целевой (tgt_text).\n",
    "# 2. Вычисляем delta = E(tgt_text) - E(src_text) в пространстве CLIP.\n",
    "# 3. Для батча сэмплов (или нескольких случайных z) получаем сгенерированные изображения,\n",
    "#    вычисляем их эмбеддинги E(I_g), и двигаем их к E(I_g) + delta (directional loss).\n",
    "# 4. Замораживаем ранние слои, чтобы «сохранить» глобальную структуру, а стили меняем на более поздних слоях.\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def freeze_layers(gen: Generator, num_freeze_layers=2):\n",
    "    \"\"\"\n",
    "    Пример: частично замораживаем слои генератора.\n",
    "    Допустим, что gen.synthesis состоит из списка блоков, где первые num_freeze_layers\n",
    "    оставляем без обучения (requires_grad=False).\n",
    "    \"\"\"\n",
    "    blocks = list(gen.synthesis.children())\n",
    "    for i, block in enumerate(blocks):\n",
    "        if i < num_freeze_layers:\n",
    "            for param in block.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "\n",
    "def compute_directional_loss(clip_model, clip_preprocess, images, src_emb, delta_t, alpha=1.0):\n",
    "    \"\"\"\n",
    "    Суть StyleGAN-NADA: двигаем эмбеддинги генерируемых изображений E(I_g)\n",
    "    в направлении delta_t = E(tgt_text) - E(src_text).\n",
    "\n",
    "    Псевдо-формула:\n",
    "       Loss = 1 - cos( E(I_g) - src_emb, delta_t )\n",
    "    Также могут использоваться разные вариации directional loss,\n",
    "    см. оригинальную статью StyleGAN-NADA.\n",
    "    \"\"\"\n",
    "    # Приводим изображения к формату (B, C, H, W), желательно 224x224\n",
    "    # Если необходимо, делаем resize\n",
    "    images_224 = torch.nn.functional.interpolate(images, size=(224, 224), mode=\"bilinear\")\n",
    "    # Преобразуем под CLIP\n",
    "    imgs_clip_ready = clip_preprocess(images_224)\n",
    "    E_ig = clip_model.encode_image(imgs_clip_ready)  # [B, dim]\n",
    "\n",
    "    # Разница E(I_g) - E(src_text)\n",
    "    diff = E_ig - src_emb\n",
    "    # Косинусная близость между diff и delta_t\n",
    "    cos_sim = F.cosine_similarity(diff, delta_t.unsqueeze(0), dim=1)\n",
    "    # Превращаем в loss (хотим максимизировать косинусную близость => минимизируем -cos_sim)\n",
    "    directional_loss = alpha * (1 - cos_sim.mean())\n",
    "\n",
    "    return directional_loss\n",
    "\n",
    "\n",
    "def train_nada(\n",
    "    network_pkl,  # Исходный путь к уже обученной StyleGAN2-ADA модели\n",
    "    src_text,  # Текст, описывающий исходный домен\n",
    "    tgt_text,  # Текст, описывающий целевой стиль/домен\n",
    "    outdir=\"nada-out\",  # Папка для результатов\n",
    "    num_steps=1000,\n",
    "    batch_size=4,\n",
    "    lr=0.002,\n",
    "    freeze_layers_num=2,\n",
    "):\n",
    "    \"\"\"\n",
    "    Дообучение StyleGAN2-ADA в стиле NADA:\n",
    "    объединяем официальный код StyleGAN2-ADA с механизмом directional CLIP loss.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    # 1. Загрузка исходной модели\n",
    "    with dnnlib.util.open_url(network_pkl) as f:\n",
    "        G = torch.load(f)[\"G_ema\"].to(device)  # Часто сеть хранится в словаре\n",
    "\n",
    "    # 2. Инициализация CLIP\n",
    "    clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "    clip_model.eval()\n",
    "\n",
    "    # 3. Получаем эмбеддинги для src_text и tgt_text\n",
    "    token_src = clip.tokenize([src_text]).to(device)\n",
    "    token_tgt = clip.tokenize([tgt_text]).to(device)\n",
    "    with torch.no_grad():\n",
    "        emb_src = clip_model.encode_text(token_src)[0]  # shape [dim]\n",
    "        emb_tgt = clip_model.encode_text(token_tgt)[0]  # shape [dim]\n",
    "    delta_t = emb_tgt - emb_src  # Вектор смещения\n",
    "\n",
    "    # 4. Замораживаем нужные слои\n",
    "    freeze_layers(G, num_freeze_layers=freeze_layers_num)\n",
    "\n",
    "    # 5. Оптимизируем только те параметры, что не заморожены\n",
    "    params = [p for p in G.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.Adam(params, lr=lr)\n",
    "\n",
    "    # 6. Основной цикл обучения\n",
    "    for step in range(num_steps):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Сэмплируем случайный шум\n",
    "        z = torch.randn(batch_size, G.z_dim, device=device)\n",
    "        # Прогоняем через мэппинг\n",
    "        ws = G.mapping(z, None)  # [batch, num_ws, w_dim]\n",
    "\n",
    "        # Генерация изображений\n",
    "        synth_images = G.synthesis(ws, noise_mode=\"const\")  # [B, 3, H, W]\n",
    "\n",
    "        # Вычисляем Directional CLIP loss\n",
    "        loss = compute_directional_loss(clip_model, clip_preprocess, synth_images, emb_src, delta_t)\n",
    "\n",
    "        # Добавляем свою регуляризацию или ADA-augmentations (см. офиц. реализацию)\n",
    "        # Например, можно смешать loss с оригинальным прогнозом дискриминатора,\n",
    "        # но в простейшем варианте оставим чистый directional loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % 50 == 0:\n",
    "            print(f\"Step {step}/{num_steps}   Loss: {loss.item():.4f}\")\n",
    "\n",
    "            # Пример сохранения одного изображения\n",
    "            with torch.no_grad():\n",
    "                out_z = torch.randn(1, G.z_dim, device=device)\n",
    "                out_ws = G.mapping(out_z, None)\n",
    "                out_img = G.synthesis(out_ws, noise_mode=\"const\")[0]\n",
    "                out_img_np = (\n",
    "                    (out_img.permute(1, 2, 0).cpu().numpy() * 127.5 + 127.5)\n",
    "                    .clip(0, 255)\n",
    "                    .astype(\"uint8\")\n",
    "                )\n",
    "                pil_img = Image.fromarray(out_img_np)\n",
    "\n",
    "                os.makedirs(outdir, exist_ok=True)\n",
    "                pil_img.save(f\"{outdir}/step_{step:04d}.png\")\n",
    "\n",
    "    # 7. Сохранение итоговых весов\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    final_pkl = os.path.join(outdir, \"stylegan2_nada_final.pkl\")\n",
    "    torch.save({\"G_ema\": G.state_dict()}, final_pkl)\n",
    "    print(f\"Done! Model saved to {final_pkl}\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Пример вызова:\n",
    "# python train_nada.py --network_pkl=/path/to/original.pkl --src_text=\"photo\" --tgt_text=\"a cubist painting\"\n",
    "# (с последующей интеграцией в ваш пайплайн StyleGAN2-ADA)\n",
    "# -------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
